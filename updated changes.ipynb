{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c950cc66",
   "metadata": {},
   "source": [
    "# （一）统计显著性检验\n",
    "2.1.2 AUC的统计不确定性（样本量有限导致的随机波动）\n",
    "\n",
    "\n",
    "2.1.8  ROC曲线的不确定性带（置信区间)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Statistical Significance Analysis ===\")\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bootstrap_auc(y_true, y_scores, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    使用Bootstrap方法计算AUC的置信区间\n",
    "    \n",
    "    参数:\n",
    "    - y_true: 真实标签\n",
    "    - y_scores: 预测分数\n",
    "    - n_bootstrap: Bootstrap采样次数\n",
    "    - confidence_level: 置信水平（默认95%）\n",
    "    \n",
    "    返回:\n",
    "    - auc_mean: AUC均值\n",
    "    - auc_ci_lower: 置信区间下界\n",
    "    - auc_ci_upper: 置信区间上界\n",
    "    - auc_std: AUC标准差\n",
    "    - bootstrap_aucs: 所有Bootstrap样本的AUC值\n",
    "    \"\"\"\n",
    "    bootstrap_aucs = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    print(f\"  Performing {n_bootstrap} bootstrap iterations...\")\n",
    "    for i in range(n_bootstrap):\n",
    "        # 有放回地随机抽样\n",
    "        indices = resample(range(n_samples), n_samples=n_samples, random_state=i)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_scores_boot = y_scores[indices]\n",
    "        \n",
    "        # 计算该样本的AUC\n",
    "        fpr_boot, tpr_boot, _ = roc_curve(y_true_boot, y_scores_boot)\n",
    "        auc_boot = auc(fpr_boot, tpr_boot)\n",
    "        bootstrap_aucs.append(auc_boot)\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"  Progress: {i+1}/{n_bootstrap} iterations completed\")\n",
    "    \n",
    "    bootstrap_aucs = np.array(bootstrap_aucs)\n",
    "    auc_mean = np.mean(bootstrap_aucs)\n",
    "    auc_std = np.std(bootstrap_aucs)\n",
    "    \n",
    "    # 计算置信区间（使用百分位数方法）\n",
    "    alpha = 1 - confidence_level\n",
    "    auc_ci_lower = np.percentile(bootstrap_aucs, 100 * alpha / 2)\n",
    "    auc_ci_upper = np.percentile(bootstrap_aucs, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return auc_mean, auc_ci_lower, auc_ci_upper, auc_std, bootstrap_aucs\n",
    "\n",
    "\n",
    "def bootstrap_roc_curve(y_true, y_scores, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    使用Bootstrap方法计算ROC曲线的置信区间\n",
    "    \n",
    "    返回:\n",
    "    - mean_fpr: 统一的FPR网格\n",
    "    - mean_tpr: 平均TPR\n",
    "    - tpr_ci_lower: TPR置信区间下界\n",
    "    - tpr_ci_upper: TPR置信区间上界\n",
    "    \"\"\"\n",
    "    print(f\"  Computing ROC curve confidence bands with {n_bootstrap} bootstrap samples...\")\n",
    "    \n",
    "    # 创建统一的FPR网格（0到1之间均匀分布）\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    \n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Bootstrap采样\n",
    "        indices = resample(range(n_samples), n_samples=n_samples, random_state=i)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_scores_boot = y_scores[indices]\n",
    "        \n",
    "        # 计算ROC曲线\n",
    "        fpr_boot, tpr_boot, _ = roc_curve(y_true_boot, y_scores_boot)\n",
    "        \n",
    "        # 插值到统一的FPR网格\n",
    "        tpr_interp = np.interp(mean_fpr, fpr_boot, tpr_boot)\n",
    "        tpr_interp[0] = 0.0  # 确保起点是(0,0)\n",
    "        tprs.append(tpr_interp)\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"  Progress: {i+1}/{n_bootstrap} ROC curves computed\")\n",
    "    \n",
    "    tprs = np.array(tprs)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0  # 确保终点是(1,1)\n",
    "    \n",
    "    # 计算置信区间\n",
    "    alpha = 1 - confidence_level\n",
    "    tpr_ci_lower = np.percentile(tprs, 100 * alpha / 2, axis=0)\n",
    "    tpr_ci_upper = np.percentile(tprs, 100 * (1 - alpha / 2), axis=0)\n",
    "    \n",
    "    return mean_fpr, mean_tpr, tpr_ci_lower, tpr_ci_upper\n",
    "\n",
    "\n",
    "# --- 1. 计算MLP分类器的AUC置信区间 ---\n",
    "print(\"\\n1. Calculating AUC confidence interval for MLP Classifier...\")\n",
    "auc_mlp_mean, auc_mlp_ci_lower, auc_mlp_ci_upper, auc_mlp_std, bootstrap_aucs_mlp = \\\n",
    "    bootstrap_auc(y_test, y_scores_mlp, n_bootstrap=1000)\n",
    "\n",
    "print(f\"\\nMLP Classifier AUC Statistics:\")\n",
    "print(f\"  Original AUC: {roc_auc_mlp:.4f}\")\n",
    "print(f\"  Bootstrap Mean AUC: {auc_mlp_mean:.4f} ± {auc_mlp_std:.4f}\")\n",
    "print(f\"  95% Confidence Interval: [{auc_mlp_ci_lower:.4f}, {auc_mlp_ci_upper:.4f}]\")\n",
    "print(f\"  CI Width: {auc_mlp_ci_upper - auc_mlp_ci_lower:.4f}\")\n",
    "\n",
    "# --- 2. 计算截断均值法的AUC置信区间 ---\n",
    "print(\"\\n2. Calculating AUC confidence interval for Truncated Mean method...\")\n",
    "auc_tm_mean, auc_tm_ci_lower, auc_tm_ci_upper, auc_tm_std, bootstrap_aucs_tm = \\\n",
    "    bootstrap_auc(y_true_truncated, y_scores_truncated, n_bootstrap=1000)\n",
    "\n",
    "print(f\"\\nTruncated Mean AUC Statistics:\")\n",
    "print(f\"  Original AUC: {roc_auc_truncated:.4f}\")\n",
    "print(f\"  Bootstrap Mean AUC: {auc_tm_mean:.4f} ± {auc_tm_std:.4f}\")\n",
    "print(f\"  95% Confidence Interval: [{auc_tm_ci_lower:.4f}, {auc_tm_ci_upper:.4f}]\")\n",
    "print(f\"  CI Width: {auc_tm_ci_upper - auc_tm_ci_lower:.4f}\")\n",
    "\n",
    "# --- 3. 统计显著性检验 ---\n",
    "print(\"\\n3. Statistical Significance Test:\")\n",
    "print(f\"  AUC Improvement: {auc_mlp_mean - auc_tm_mean:.4f}\")\n",
    "print(f\"  Relative Improvement: {(auc_mlp_mean - auc_tm_mean) / auc_tm_mean * 100:.2f}%\")\n",
    "\n",
    "# 检查置信区间是否重叠\n",
    "ci_overlap = not (auc_mlp_ci_lower > auc_tm_ci_upper or auc_tm_ci_lower > auc_mlp_ci_upper)\n",
    "print(f\"  95% CI Overlap: {'Yes' if ci_overlap else 'No'}\")\n",
    "\n",
    "if not ci_overlap:\n",
    "    print(\"  ✓ The improvement is statistically significant at 95% confidence level!\")\n",
    "else:\n",
    "    # 使用配对t检验（因为是同一批数据）\n",
    "    # 注意：这里使用的是bootstrap分布进行t检验\n",
    "    t_statistic, p_value = stats.ttest_ind(bootstrap_aucs_mlp, bootstrap_aucs_tm)\n",
    "    print(f\"  Independent t-test p-value: {p_value:.6f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  ✓ The improvement is statistically significant (p < 0.05)!\")\n",
    "    else:\n",
    "        print(\"  ✗ The improvement is NOT statistically significant (p >= 0.05)\")\n",
    "\n",
    "# --- 4. 可视化AUC分布 ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(bootstrap_aucs_mlp, bins=50, alpha=0.6, color='darkorange', label='MLP', density=True)\n",
    "plt.hist(bootstrap_aucs_tm, bins=50, alpha=0.6, color='blue', label='Truncated Mean', density=True)\n",
    "plt.axvline(auc_mlp_mean, color='darkorange', linestyle='--', linewidth=2, label=f'MLP Mean: {auc_mlp_mean:.4f}')\n",
    "plt.axvline(auc_tm_mean, color='blue', linestyle='--', linewidth=2, label=f'TM Mean: {auc_tm_mean:.4f}')\n",
    "plt.xlabel('AUC')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Bootstrap Distribution of AUC Values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "methods = ['MLP Classifier', 'Truncated Mean']\n",
    "means = [auc_mlp_mean, auc_tm_mean]\n",
    "ci_lower = [auc_mlp_ci_lower, auc_tm_ci_lower]\n",
    "ci_upper = [auc_mlp_ci_upper, auc_tm_ci_upper]\n",
    "errors = [[auc_mlp_mean - auc_mlp_ci_lower], [auc_mlp_ci_upper - auc_mlp_mean]]\n",
    "\n",
    "x_pos = np.arange(len(methods))\n",
    "plt.bar(x_pos, means, yerr=[[means[i] - ci_lower[i] for i in range(2)], \n",
    "                              [ci_upper[i] - means[i] for i in range(2)]], \n",
    "        align='center', alpha=0.7, ecolor='black', capsize=10,\n",
    "        color=['darkorange', 'blue'])\n",
    "plt.ylabel('AUC')\n",
    "plt.xticks(x_pos, methods)\n",
    "plt.title('AUC Comparison with 95% Confidence Intervals')\n",
    "plt.ylim([0.75, 0.85])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 添加数值标签\n",
    "for i, (method, mean, lower, upper) in enumerate(zip(methods, means, ci_lower, ci_upper)):\n",
    "    plt.text(i, upper + 0.005, f'{mean:.4f}\\n±{(upper-lower)/2:.4f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('auc_statistical_significance_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 5. 计算并绘制带置信区间的ROC曲线 ---\n",
    "print(\"\\n4. Computing ROC curves with confidence bands...\")\n",
    "\n",
    "# 为MLP计算ROC置信带\n",
    "mean_fpr_mlp, mean_tpr_mlp, tpr_ci_lower_mlp, tpr_ci_upper_mlp = \\\n",
    "    bootstrap_roc_curve(y_test, y_scores_mlp, n_bootstrap=1000)\n",
    "\n",
    "# 为截断均值法计算ROC置信带\n",
    "mean_fpr_tm, mean_tpr_tm, tpr_ci_lower_tm, tpr_ci_upper_tm = \\\n",
    "    bootstrap_roc_curve(y_true_truncated, y_scores_truncated, n_bootstrap=1000)\n",
    "\n",
    "# --- 6. 绘制增强版ROC曲线（带置信区间） ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 原始ROC曲线作为对比\n",
    "plt.plot(fpr_mlp, tpr_mlp, color='darkorange', lw=1, alpha=0.3, \n",
    "         label=f'MLP Original (AUC = {roc_auc_mlp:.4f})')\n",
    "plt.plot(fpr_truncated, tpr_truncated, color='blue', lw=1, alpha=0.3,\n",
    "         label=f'TM Original (AUC = {roc_auc_truncated:.4f})')\n",
    "\n",
    "# 平均ROC曲线（粗线）\n",
    "plt.plot(mean_fpr_mlp, mean_tpr_mlp, color='darkorange', lw=3,\n",
    "         label=f'MLP Mean (AUC = {auc_mlp_mean:.4f} [{auc_mlp_ci_lower:.4f}, {auc_mlp_ci_upper:.4f}])')\n",
    "plt.plot(mean_fpr_tm, mean_tpr_tm, color='blue', lw=3, linestyle='--',\n",
    "         label=f'TM Mean (AUC = {auc_tm_mean:.4f} [{auc_tm_ci_lower:.4f}, {auc_tm_ci_upper:.4f}])')\n",
    "\n",
    "# 95%置信区间（阴影带）\n",
    "plt.fill_between(mean_fpr_mlp, tpr_ci_lower_mlp, tpr_ci_upper_mlp, \n",
    "                 color='darkorange', alpha=0.2, label='MLP 95% CI')\n",
    "plt.fill_between(mean_fpr_tm, tpr_ci_lower_tm, tpr_ci_upper_tm, \n",
    "                 color='blue', alpha=0.2, label='TM 95% CI')\n",
    "\n",
    "# 对角线（随机猜测）\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle=':', label='Random Guess')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (Proton identified as Pion)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Proton identified as Proton)', fontsize=12)\n",
    "plt.title(f'ROC Curves with 95% Confidence Intervals\\n(Pion vs Proton, p={MOMENTUM_OVERLAP_MIN}-{MOMENTUM_OVERLAP_MAX} GeV/c)', \n",
    "          fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 添加文本框显示统计信息\n",
    "textstr = f'Statistical Significance:\\n'\n",
    "textstr += f'ΔAUC = {auc_mlp_mean - auc_tm_mean:.4f}\\n'\n",
    "textstr += f'Relative Improvement = {(auc_mlp_mean - auc_tm_mean) / auc_tm_mean * 100:.2f}%\\n'\n",
    "if not ci_overlap:\n",
    "    textstr += f'95% CIs do NOT overlap ✓'\n",
    "else:\n",
    "    textstr += f'p-value = {p_value:.4f}'\n",
    "    \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "plt.text(0.60, 0.15, textstr, transform=plt.gca().transAxes, fontsize=11,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_with_confidence_intervals.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Analysis Complete ===\")\n",
    "print(\"Generated files:\")\n",
    "print(\"  1. auc_statistical_significance_analysis.png\")\n",
    "print(\"  2. roc_curves_with_confidence_intervals.png\")\n",
    "\n",
    "# --- 7. 生成详细的统计报告 ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL STATISTICAL REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMethod Comparison (p = {MOMENTUM_OVERLAP_MIN}-{MOMENTUM_OVERLAP_MAX} GeV/c):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Method':<20} {'AUC':<12} {'95% CI':<25} {'Std Dev':<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'MLP Classifier':<20} {auc_mlp_mean:.4f}      [{auc_mlp_ci_lower:.4f}, {auc_mlp_ci_upper:.4f}]    {auc_mlp_std:.4f}\")\n",
    "print(f\"{'Truncated Mean':<20} {auc_tm_mean:.4f}      [{auc_tm_ci_lower:.4f}, {auc_tm_ci_upper:.4f}]    {auc_tm_std:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nImprovement: {auc_mlp_mean - auc_tm_mean:.4f} ({(auc_mlp_mean - auc_tm_mean) / auc_tm_mean * 100:.2f}%)\")\n",
    "print(f\"Significance: {'YES (CIs do not overlap)' if not ci_overlap else f'p-value = {p_value:.4f}'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a440a",
   "metadata": {},
   "source": [
    "# （二）特征理解\n",
    "2.1.6特征重要性分析\n",
    "\n",
    "\n",
    "2.2.4输入特征分布可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ed17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUPPLEMENTARY ANALYSIS: Feature Importance and Distribution\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# --- 第一部分：特征重要性分析（Permutation Importance） ---\n",
    "print(\"\\n[1/2] Computing Feature Importance using Permutation Importance...\")\n",
    "print(\"This method measures how much the model performance drops when a feature is randomly shuffled.\")\n",
    "\n",
    "# 创建一个符合sklearn规范的包装器\n",
    "class PyTorchClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"包装PyTorch模型使其完全兼容sklearn\"\"\"\n",
    "    def __init__(self, model, device, scaler):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.scaler = scaler\n",
    "        self.classes_ = np.array([0, 1])  # 二分类\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"占位符fit方法（模型已经训练好了）\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        with torch.no_grad():\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "            outputs = self.model(X_tensor).cpu().numpy().flatten()\n",
    "        # 返回[P(class=0), P(class=1)]\n",
    "        proba = np.column_stack([1 - outputs, outputs])\n",
    "        return proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] > 0.5).astype(int)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"计算准确率\"\"\"\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n",
    "# 创建包装器\n",
    "wrapped_model = PyTorchClassifierWrapper(model_classifier, device, scaler_classifier)\n",
    "\n",
    "# 计算排列重要性（使用测试集）\n",
    "print(\"  Running permutation importance (this may take a few minutes)...\")\n",
    "perm_importance = permutation_importance(\n",
    "    wrapped_model, \n",
    "    X_test,  # 使用未缩放的测试数据\n",
    "    y_test, \n",
    "    n_repeats=30,  # 重复30次以获得稳定结果\n",
    "    random_state=42,\n",
    "    scoring='roc_auc',  # 使用AUC作为评分标准\n",
    "    n_jobs=-1  # 使用所有CPU核心\n",
    ")\n",
    "\n",
    "# 提取特征名称\n",
    "# 前50个是排序后的dE/dx值，后5个是统计特征\n",
    "feature_names = [f'dE/dx_{i+1}' for i in range(NUM_DE_DX_VALUES)] + \\\n",
    "                ['Mean', 'Std Dev', 'Median', 'Skewness', 'Kurtosis']\n",
    "\n",
    "# 创建特征重要性DataFrame\n",
    "importance_df = {\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "}\n",
    "\n",
    "# 按重要性排序\n",
    "sorted_idx = np.argsort(perm_importance.importances_mean)[::-1]\n",
    "\n",
    "print(\"\\n  Top 15 Most Important Features:\")\n",
    "print(\"  \" + \"-\"*70)\n",
    "print(f\"  {'Rank':<6} {'Feature':<20} {'Importance':<15} {'Std Dev':<10}\")\n",
    "print(\"  \" + \"-\"*70)\n",
    "for i, idx in enumerate(sorted_idx[:15], 1):\n",
    "    print(f\"  {i:<6} {feature_names[idx]:<20} {perm_importance.importances_mean[idx]:.6f}     \"\n",
    "          f\"±{perm_importance.importances_std[idx]:.6f}\")\n",
    "\n",
    "# 分析统计特征的重要性\n",
    "stat_features_idx = list(range(NUM_DE_DX_VALUES, NUM_DE_DX_VALUES + 5))\n",
    "stat_features_importance = perm_importance.importances_mean[stat_features_idx]\n",
    "stat_features_names = ['Mean', 'Std Dev', 'Median', 'Skewness', 'Kurtosis']\n",
    "\n",
    "print(\"\\n  Statistical Features Importance Summary:\")\n",
    "print(\"  \" + \"-\"*70)\n",
    "for name, imp, std in zip(stat_features_names, stat_features_importance, \n",
    "                           perm_importance.importances_std[stat_features_idx]):\n",
    "    print(f\"  {name:<15} Importance: {imp:.6f} ± {std:.6f}\")\n",
    "print(\"  \" + \"-\"*70)\n",
    "\n",
    "# ====== 图1：Top 20 特征重要性（单图） ======\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "top_n = 20\n",
    "top_indices = sorted_idx[:top_n]\n",
    "y_pos = np.arange(top_n)\n",
    "colors = ['red' if idx >= NUM_DE_DX_VALUES else 'steelblue' for idx in top_indices]\n",
    "\n",
    "ax.barh(y_pos, perm_importance.importances_mean[top_indices], \n",
    "        xerr=perm_importance.importances_std[top_indices],\n",
    "        color=colors, alpha=0.7, capsize=3)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([feature_names[i] for i in top_indices])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Permutation Importance (ΔAUC)', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Most Important Features for Pion/Proton Classification', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 创建图例\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='red', alpha=0.7, label='Statistical Features'),\n",
    "                   Patch(facecolor='steelblue', alpha=0.7, label='dE/dx Values')]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n  ✓ Feature importance plot saved as 'feature_importance_analysis.png'\")\n",
    "\n",
    "# --- 第二部分：特征分布可视化 ---\n",
    "print(\"\\n[2/2] Visualizing Statistical Feature Distributions for Pions vs Protons...\")\n",
    "\n",
    "# 从测试集中提取π介子和质子的数据\n",
    "pion_mask_test = y_test == 0\n",
    "proton_mask_test = y_test == 1\n",
    "\n",
    "# 提取统计特征（最后5列）\n",
    "stat_features_test = X_test[:, -5:]  # 未缩放的测试数据的最后5列\n",
    "stat_features_pions = stat_features_test[pion_mask_test]\n",
    "stat_features_protons = stat_features_test[proton_mask_test]\n",
    "\n",
    "print(f\"  Number of pions in test set: {np.sum(pion_mask_test)}\")\n",
    "print(f\"  Number of protons in test set: {np.sum(proton_mask_test)}\")\n",
    "\n",
    "# ====== 图2：5个统计特征的分布对比 ======\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "stat_feature_labels = ['Mean dE/dx', 'Std Dev of dE/dx', 'Median dE/dx', \n",
    "                       'Skewness of dE/dx', 'Kurtosis of dE/dx']\n",
    "\n",
    "for i, (label, importance) in enumerate(zip(stat_feature_labels, stat_features_importance)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # 绘制直方图\n",
    "    ax.hist(stat_features_pions[:, i], bins=50, alpha=0.5, label='Pion (π)', \n",
    "            color='blue', density=True)\n",
    "    ax.hist(stat_features_protons[:, i], bins=50, alpha=0.5, label='Proton (p)', \n",
    "            color='red', density=True)\n",
    "    \n",
    "    # 添加均值线\n",
    "    pion_mean = np.mean(stat_features_pions[:, i])\n",
    "    proton_mean = np.mean(stat_features_protons[:, i])\n",
    "    ax.axvline(pion_mean, color='blue', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    ax.axvline(proton_mean, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # 计算分离度（效应量：Cohen's d）\n",
    "    pooled_std = np.sqrt((np.std(stat_features_pions[:, i])**2 + \n",
    "                          np.std(stat_features_protons[:, i])**2) / 2)\n",
    "    cohens_d = (proton_mean - pion_mean) / pooled_std\n",
    "    \n",
    "    # 进行Kolmogorov-Smirnov检验\n",
    "    ks_statistic, ks_pvalue = scipy_stats.ks_2samp(stat_features_pions[:, i], \n",
    "                                                     stat_features_protons[:, i])\n",
    "    \n",
    "    ax.set_xlabel(label, fontsize=11)\n",
    "    ax.set_ylabel('Probability Density', fontsize=11)\n",
    "    ax.set_title(f'{label}\\nImportance: {importance:.5f} | Cohen\\'s d: {cohens_d:.3f}', \n",
    "                 fontsize=10)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加文本框显示统计信息\n",
    "    textstr = f'Pion μ={pion_mean:.3f}\\nProton μ={proton_mean:.3f}\\n'\n",
    "    textstr += f'KS test p<{ks_pvalue:.1e}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.7)\n",
    "    ax.text(0.98, 0.97, textstr, transform=ax.transAxes, fontsize=8,\n",
    "            verticalalignment='top', horizontalalignment='right', bbox=props)\n",
    "\n",
    "# 删除第6个子图（因为只有5个特征）\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle(f'Statistical Feature Distributions: Pions vs Protons\\n'\n",
    "             f'(Momentum range: {MOMENTUM_OVERLAP_MIN}-{MOMENTUM_OVERLAP_MAX} GeV/c)', \n",
    "             fontsize=14, y=0.995)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.savefig('statistical_features_distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"  ✓ Feature distribution plot saved as 'statistical_features_distribution_comparison.png'\")\n",
    "\n",
    "# 计算每个统计特征的Cohen's d\n",
    "cohens_d_values = []\n",
    "for i in range(5):\n",
    "    pion_mean = np.mean(stat_features_pions[:, i])\n",
    "    proton_mean = np.mean(stat_features_protons[:, i])\n",
    "    pooled_std = np.sqrt((np.std(stat_features_pions[:, i])**2 + \n",
    "                          np.std(stat_features_protons[:, i])**2) / 2)\n",
    "    cohens_d = (proton_mean - pion_mean) / pooled_std\n",
    "    cohens_d_values.append(cohens_d)\n",
    "\n",
    "print(\"a) Mean dE/dx:\")\n",
    "print(f\"   - Importance: {stat_features_importance[0]:.6f}\")\n",
    "print(f\"   - Cohen's d: {cohens_d_values[0]:.3f}\")\n",
    "print(\"   - Physics: Central tendency of energy loss, directly related to Bethe-Bloch\")\n",
    "print()\n",
    "print(\"b) Standard Deviation:\")\n",
    "print(f\"   - Importance: {stat_features_importance[1]:.6f}\")\n",
    "print(f\"   - Cohen's d: {cohens_d_values[1]:.3f}\")\n",
    "print(\"   - Physics: Captures fluctuations in energy loss (Landau distribution)\")\n",
    "print()\n",
    "print(\"c) Median dE/dx:\")\n",
    "print(f\"   - Importance: {stat_features_importance[2]:.6f}\")\n",
    "print(f\"   - Cohen's d: {cohens_d_values[2]:.3f}\")\n",
    "print(\"   - Physics: Robust central measure, less affected by outliers\")\n",
    "print()\n",
    "print(\"d) Skewness:\")\n",
    "print(f\"   - Importance: {stat_features_importance[3]:.6f}\")\n",
    "print(f\"   - Cohen's d: {cohens_d_values[3]:.3f}\")\n",
    "print(\"   - Physics: Asymmetry of energy loss distribution (Landau tail)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf02ed",
   "metadata": {},
   "source": [
    "# （三）模型评估扩展\n",
    "2.1.4扩展到其他粒子对和动量范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTENDED ANALYSIS: Multi-Particle-Pair & Multi-Momentum-Range Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. 定义要测试的粒子对和动量范围 ---\n",
    "# 根据数据集中的质量值定义粒子\n",
    "PARTICLE_DEFINITIONS = {\n",
    "    'Electron': 0.001,\n",
    "    'Pion': 0.139,\n",
    "    'Kaon': 0.494,\n",
    "    'Proton': 0.937\n",
    "}\n",
    "\n",
    "# 定义要测试的粒子对（选择物理上重要的组合）\n",
    "PARTICLE_PAIRS = [\n",
    "    ('Pion', 'Proton'),      # 原始测试\n",
    "    ('Kaon', 'Proton'),      # 高动量区域重要\n",
    "    ('Pion', 'Kaon'),        # 中等动量区域挑战\n",
    "    ('Electron', 'Pion'),    # 低动量区域\n",
    "]\n",
    "\n",
    "# 定义多个动量范围（覆盖不同物理区域）\n",
    "MOMENTUM_RANGES = [\n",
    "    (0.3, 0.6),   # 低动量（粒子明显分离）\n",
    "    (0.6, 1.0),   # 中低动量\n",
    "    (1.0, 1.5),   # 中等动量（原始范围的前半段）\n",
    "    (1.5, 2.0),   # 中高动量（原始范围的后半段）\n",
    "    (2.0, 2.5),   # 高动量（分离困难）\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting Configuration:\")\n",
    "print(f\"  Particle pairs: {len(PARTICLE_PAIRS)}\")\n",
    "print(f\"  Momentum ranges: {len(MOMENTUM_RANGES)}\")\n",
    "print(f\"  Total test cases: {len(PARTICLE_PAIRS) * len(MOMENTUM_RANGES)}\")\n",
    "\n",
    "# --- 2. 准备数据提取函数 ---\n",
    "def extract_particle_pair_data(mass_data, momentum_data, X_data, y_truncated_data,\n",
    "                                particle1_name, particle2_name, p_min, p_max,\n",
    "                                particle_defs=PARTICLE_DEFINITIONS):\n",
    "    \"\"\"\n",
    "    从全量数据中提取特定粒子对在特定动量范围的数据\n",
    "    \n",
    "    返回: X_pair, y_pair, p_pair, dedx_pair, n_particle1, n_particle2\n",
    "    \"\"\"\n",
    "    mass1 = particle_defs[particle1_name]\n",
    "    mass2 = particle_defs[particle2_name]\n",
    "    \n",
    "    # 筛选粒子1\n",
    "    mask1 = (mass_data == mass1) & (momentum_data >= p_min) & (momentum_data < p_max)\n",
    "    X1 = X_data[mask1]\n",
    "    p1 = momentum_data[mask1]\n",
    "    dedx1 = y_truncated_data[mask1]\n",
    "    \n",
    "    # 筛选粒子2\n",
    "    mask2 = (mass_data == mass2) & (momentum_data >= p_min) & (momentum_data < p_max)\n",
    "    X2 = X_data[mask2]\n",
    "    p2 = momentum_data[mask2]\n",
    "    dedx2 = y_truncated_data[mask2]\n",
    "    \n",
    "    # 检查数据量\n",
    "    n1, n2 = len(X1), len(X2)\n",
    "    if n1 < 100 or n2 < 100:\n",
    "        return None, None, None, None, n1, n2\n",
    "    \n",
    "    # 合并数据 (粒子1标签为0，粒子2标签为1)\n",
    "    X_pair = np.vstack((X1, X2))\n",
    "    y_pair = np.hstack((np.zeros(n1), np.ones(n2)))\n",
    "    p_pair = np.hstack((p1, p2))\n",
    "    dedx_pair = np.hstack((dedx1, dedx2))\n",
    "    \n",
    "    return X_pair, y_pair, p_pair, dedx_pair, n1, n2\n",
    "\n",
    "\n",
    "# --- 3. 为每个测试用例训练和评估模型 ---\n",
    "results_comprehensive = []\n",
    "\n",
    "print(\"\\nStarting comprehensive evaluation...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "test_case_count = 0\n",
    "for (particle1, particle2), (p_min, p_max) in itertools.product(PARTICLE_PAIRS, MOMENTUM_RANGES):\n",
    "    test_case_count += 1\n",
    "    print(f\"\\n[{test_case_count}/{len(PARTICLE_PAIRS) * len(MOMENTUM_RANGES)}] \"\n",
    "          f\"Testing: {particle1} vs {particle2}, p ∈ [{p_min}, {p_max}) GeV/c\")\n",
    "    \n",
    "    # 提取数据\n",
    "    X_pair, y_pair, p_pair, dedx_pair, n1, n2 = extract_particle_pair_data(\n",
    "        mass_data_full, momentum_data_full, X_data_full, y_truncated_mean_full,\n",
    "        particle1, particle2, p_min, p_max\n",
    "    )\n",
    "    \n",
    "    if X_pair is None:\n",
    "        print(f\"  ✗ Insufficient data: {particle1}={n1}, {particle2}={n2} (need ≥100 each)\")\n",
    "        results_comprehensive.append({\n",
    "            'Particle_Pair': f'{particle1}-{particle2}',\n",
    "            'Momentum_Range': f'{p_min}-{p_max}',\n",
    "            'N_Particle1': n1,\n",
    "            'N_Particle2': n2,\n",
    "            'Status': 'Insufficient Data',\n",
    "            'MLP_AUC': np.nan,\n",
    "            'TruncMean_AUC': np.nan,\n",
    "            'AUC_Improvement': np.nan,\n",
    "            'MLP_Accuracy': np.nan,\n",
    "            'TruncMean_Accuracy': np.nan\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Data: {particle1}={n1}, {particle2}={n2}\")\n",
    "    \n",
    "    # 数据标准化和划分\n",
    "    scaler_temp = StandardScaler()\n",
    "    X_pair_scaled = scaler_temp.fit_transform(X_pair)\n",
    "    \n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp, \\\n",
    "    p_train_temp, p_test_temp, dedx_train_temp, dedx_test_temp = train_test_split(\n",
    "        X_pair_scaled, y_pair, p_pair, dedx_pair,\n",
    "        test_size=0.2, random_state=42, stratify=y_pair\n",
    "    )\n",
    "    \n",
    "    # 转换为Tensor\n",
    "    X_train_tensor_temp = torch.tensor(X_train_temp, dtype=torch.float32).to(device)\n",
    "    y_train_tensor_temp = torch.tensor(y_train_temp, dtype=torch.float32).to(device).reshape(-1, 1)\n",
    "    X_test_tensor_temp = torch.tensor(X_test_temp, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # 创建DataLoader\n",
    "    train_dataset_temp = TensorDataset(X_train_tensor_temp, y_train_tensor_temp)\n",
    "    train_loader_temp = DataLoader(train_dataset_temp, batch_size=256, shuffle=True)\n",
    "    \n",
    "    # 训练新的MLP模型（使用相同架构）\n",
    "    model_temp = MLPClassifierOptimized(input_size, hidden_size_classifier).to(device)\n",
    "    criterion_temp = nn.BCELoss()\n",
    "    optimizer_temp = optim.Adam(model_temp.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # 快速训练（50个epoch，无早停以节省时间）\n",
    "    print(\"  Training MLP...\", end=\" \")\n",
    "    for epoch in range(50):\n",
    "        model_temp.train()\n",
    "        for inputs, labels in train_loader_temp:\n",
    "            optimizer_temp.zero_grad()\n",
    "            outputs = model_temp(inputs)\n",
    "            loss = criterion_temp(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_temp.step()\n",
    "    \n",
    "    # MLP预测和评估\n",
    "    model_temp.eval()\n",
    "    with torch.no_grad():\n",
    "        y_scores_mlp_temp = model_temp(X_test_tensor_temp).cpu().numpy().flatten()\n",
    "    \n",
    "    auc_mlp_temp = roc_auc_score(y_test_temp, y_scores_mlp_temp)\n",
    "    y_pred_mlp_temp = (y_scores_mlp_temp >= 0.5).astype(int)\n",
    "    acc_mlp_temp = accuracy_score(y_test_temp, y_pred_mlp_temp)\n",
    "    \n",
    "    # 截断均值法评估（使用dedx作为分数）\n",
    "    auc_truncmean_temp = roc_auc_score(y_test_temp, dedx_test_temp)\n",
    "    # 使用中位数作为阈值进行二分类\n",
    "    threshold_truncmean = np.median(dedx_test_temp)\n",
    "    y_pred_truncmean_temp = (dedx_test_temp >= threshold_truncmean).astype(int)\n",
    "    acc_truncmean_temp = accuracy_score(y_test_temp, y_pred_truncmean_temp)\n",
    "    \n",
    "    improvement = auc_mlp_temp - auc_truncmean_temp\n",
    "    \n",
    "    print(f\"Done! MLP AUC={auc_mlp_temp:.4f}, TruncMean AUC={auc_truncmean_temp:.4f}, \"\n",
    "          f\"Δ={improvement:.4f} ({improvement/auc_truncmean_temp*100:+.2f}%)\")\n",
    "    \n",
    "    # 保存结果\n",
    "    results_comprehensive.append({\n",
    "        'Particle_Pair': f'{particle1}-{particle2}',\n",
    "        'Momentum_Range': f'{p_min}-{p_max}',\n",
    "        'N_Particle1': n1,\n",
    "        'N_Particle2': n2,\n",
    "        'Status': 'Success',\n",
    "        'MLP_AUC': auc_mlp_temp,\n",
    "        'TruncMean_AUC': auc_truncmean_temp,\n",
    "        'AUC_Improvement': improvement,\n",
    "        'Relative_Improvement_%': improvement / auc_truncmean_temp * 100,\n",
    "        'MLP_Accuracy': acc_mlp_temp,\n",
    "        'TruncMean_Accuracy': acc_truncmean_temp\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE EVALUATION COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 4. 创建结果DataFrame并分析 ---\n",
    "df_results = pd.DataFrame(results_comprehensive)\n",
    "\n",
    "# 分离成功和失败的测试用例\n",
    "df_success = df_results[df_results['Status'] == 'Success'].copy()\n",
    "df_failed = df_results[df_results['Status'] != 'Success'].copy()\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"  Successful tests: {len(df_success)}/{len(df_results)}\")\n",
    "print(f\"  Failed tests (insufficient data): {len(df_failed)}\")\n",
    "\n",
    "if len(df_success) > 0:\n",
    "    print(f\"\\nOverall Statistics (successful tests only):\")\n",
    "    print(f\"  Average MLP AUC: {df_success['MLP_AUC'].mean():.4f} ± {df_success['MLP_AUC'].std():.4f}\")\n",
    "    print(f\"  Average TruncMean AUC: {df_success['TruncMean_AUC'].mean():.4f} ± {df_success['TruncMean_AUC'].std():.4f}\")\n",
    "    print(f\"  Average Improvement: {df_success['AUC_Improvement'].mean():.4f} ({df_success['Relative_Improvement_%'].mean():.2f}%)\")\n",
    "    print(f\"  MLP wins in: {(df_success['AUC_Improvement'] > 0).sum()}/{len(df_success)} cases\")\n",
    "    \n",
    "    # 保存详细结果到CSV\n",
    "    df_results.to_csv('comprehensive_evaluation_results.csv', index=False)\n",
    "    print(f\"\\n✓ Detailed results saved to 'comprehensive_evaluation_results.csv'\")\n",
    "\n",
    "# --- 5. 可视化：热图矩阵（最核心的图） ---\n",
    "print(\"\\nGenerating performance heatmap...\")\n",
    "\n",
    "# 为热图准备数据（只包含成功的测试）\n",
    "if len(df_success) > 0:\n",
    "    # 创建数据透视表：粒子对 vs 动量范围\n",
    "    heatmap_data_improvement = df_success.pivot_table(\n",
    "        values='AUC_Improvement', \n",
    "        index='Particle_Pair', \n",
    "        columns='Momentum_Range',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    heatmap_data_mlp_auc = df_success.pivot_table(\n",
    "        values='MLP_AUC',\n",
    "        index='Particle_Pair',\n",
    "        columns='Momentum_Range',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # 绘制热图\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # 子图1：AUC改进热图（最关键）\n",
    "    sns.heatmap(heatmap_data_improvement, annot=True, fmt='.4f', cmap='RdYlGn',\n",
    "                center=0, vmin=-0.05, vmax=0.05, cbar_kws={'label': 'ΔAUC (MLP - TruncMean)'},\n",
    "                linewidths=0.5, ax=ax1)\n",
    "    ax1.set_title('MLP Performance Improvement Over Truncated Mean\\n(Green=Better, Red=Worse)', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('Momentum Range (GeV/c)', fontsize=11)\n",
    "    ax1.set_ylabel('Particle Pair', fontsize=11)\n",
    "    \n",
    "    # 子图2：MLP绝对AUC值热图\n",
    "    sns.heatmap(heatmap_data_mlp_auc, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                vmin=0.5, vmax=1.0, cbar_kws={'label': 'MLP AUC'},\n",
    "                linewidths=0.5, ax=ax2)\n",
    "    ax2.set_title('MLP Absolute Performance (AUC)\\n(Darker=Better Classification)', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlabel('Momentum Range (GeV/c)', fontsize=11)\n",
    "    ax2.set_ylabel('Particle Pair', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Performance heatmap saved as 'comprehensive_performance_heatmap.png'\")\n",
    "\n",
    "# --- 6. 可视化：条形图对比（按粒子对分组） ---\n",
    "if len(df_success) > 0:\n",
    "    print(\"\\nGenerating grouped bar chart...\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # 按粒子对分组\n",
    "    particle_pairs_unique = df_success['Particle_Pair'].unique()\n",
    "    x = np.arange(len(particle_pairs_unique))\n",
    "    width = 0.35\n",
    "    \n",
    "    # 计算每个粒子对的平均AUC\n",
    "    mlp_avg = [df_success[df_success['Particle_Pair']==pair]['MLP_AUC'].mean() \n",
    "               for pair in particle_pairs_unique]\n",
    "    truncmean_avg = [df_success[df_success['Particle_Pair']==pair]['TruncMean_AUC'].mean() \n",
    "                     for pair in particle_pairs_unique]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, mlp_avg, width, label='MLP Classifier', \n",
    "                   color='darkorange', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, truncmean_avg, width, label='Truncated Mean', \n",
    "                   color='steelblue', alpha=0.8)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Particle Pair', fontsize=12)\n",
    "    ax.set_ylabel('Average AUC (across all momentum ranges)', fontsize=12)\n",
    "    ax.set_title('MLP vs Truncated Mean: Performance Comparison by Particle Pair', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(particle_pairs_unique, fontsize=10)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0.5, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_performance_by_particle_pair.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Grouped bar chart saved as 'comprehensive_performance_by_particle_pair.png'\")\n",
    "\n",
    "# --- 7. 生成最终报告 ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPREHENSIVE REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(df_success) > 0:\n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # 找出MLP表现最好和最差的场景\n",
    "    best_case = df_success.loc[df_success['AUC_Improvement'].idxmax()]\n",
    "    worst_case = df_success.loc[df_success['AUC_Improvement'].idxmin()]\n",
    "    \n",
    "    print(f\"\\n1. Best Performance Improvement:\")\n",
    "    print(f\"   Particle Pair: {best_case['Particle_Pair']}\")\n",
    "    print(f\"   Momentum Range: {best_case['Momentum_Range']} GeV/c\")\n",
    "    print(f\"   Improvement: ΔAUC = {best_case['AUC_Improvement']:.4f} ({best_case['Relative_Improvement_%']:.2f}%)\")\n",
    "    print(f\"   MLP AUC: {best_case['MLP_AUC']:.4f} vs TruncMean: {best_case['TruncMean_AUC']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n2. Worst Performance (or smallest improvement):\")\n",
    "    print(f\"   Particle Pair: {worst_case['Particle_Pair']}\")\n",
    "    print(f\"   Momentum Range: {worst_case['Momentum_Range']} GeV/c\")\n",
    "    print(f\"   Improvement: ΔAUC = {worst_case['AUC_Improvement']:.4f} ({worst_case['Relative_Improvement_%']:.2f}%)\")\n",
    "    print(f\"   MLP AUC: {worst_case['MLP_AUC']:.4f} vs TruncMean: {worst_case['TruncMean_AUC']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n3. Consistency Analysis:\")\n",
    "    positive_improvements = (df_success['AUC_Improvement'] > 0).sum()\n",
    "    print(f\"   MLP outperforms TruncMean in {positive_improvements}/{len(df_success)} scenarios\")\n",
    "    \n",
    "    significant_improvements = (df_success['AUC_Improvement'] > 0.01).sum()\n",
    "    print(f\"   Significant improvements (ΔAUC > 0.01): {significant_improvements}/{len(df_success)} scenarios\")\n",
    "    \n",
    "    # 按动量范围分析\n",
    "    print(f\"\\n4. Performance by Momentum Range:\")\n",
    "    for mom_range in df_success['Momentum_Range'].unique():\n",
    "        subset = df_success[df_success['Momentum_Range'] == mom_range]\n",
    "        avg_improvement = subset['AUC_Improvement'].mean()\n",
    "        print(f\"   {mom_range} GeV/c: Average ΔAUC = {avg_improvement:.4f} ({len(subset)} pairs tested)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONCLUSION:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    overall_improvement = df_success['AUC_Improvement'].mean()\n",
    "    if overall_improvement > 0.01:\n",
    "        print(f\"✓ The MLP classifier demonstrates CONSISTENT and SIGNIFICANT superiority\")\n",
    "        print(f\"  over the traditional truncated mean method across diverse scenarios.\")\n",
    "        print(f\"  Average improvement: ΔAUC = {overall_improvement:.4f} ({df_success['Relative_Improvement_%'].mean():.2f}%)\")\n",
    "    elif overall_improvement > 0:\n",
    "        print(f\"✓ The MLP classifier shows MODEST bukt CONSISTENT improvement\")\n",
    "        print(f\"  over the traditional method in most scenarios.\")\n",
    "        print(f\"  Average improvement: ΔAUC = {overall_improvement:.4f}\")\n",
    "    else:\n",
    "        print(f\"⚠ The MLP classifier does NOT consistently outperform the traditional method.\")\n",
    "        print(f\"  Further optimization or feature engineering may be needed.\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ Comprehensive evaluation complete!\")\n",
    "print(\"  Generated files:\")\n",
    "print(\"  - comprehensive_evaluation_results.csv\")\n",
    "print(\"  - comprehensive_performance_heatmap.png\")\n",
    "print(\"  - comprehensive_performance_by_particle_pair.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59285b50",
   "metadata": {},
   "source": [
    "# 2.1.5 与其他机器学习模型的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK ANALYSIS: MLP vs XGBoost vs Random Forest\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# --- 1. 使用原始数据（Pion vs Proton, 1.0-2.0 GeV/c）---\n",
    "print(\"\\n[Step 1/5] Using existing data from previous analysis...\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "\n",
    "# --- 2. 训练XGBoost模型 ---\n",
    "print(\"\\n[Step 2/5] Training XGBoost Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=15,\n",
    "    tree_method='hist',  # 使用直方图优化加速\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'  # GPU加速\n",
    ")\n",
    "\n",
    "# 训练XGBoost（使用验证集进行早停）\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_train_time = time.time() - start_time\n",
    "print(f\"  ✓ XGBoost training completed in {xgb_train_time:.2f}s\")\n",
    "print(f\"  Best iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "# XGBoost预测\n",
    "y_scores_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_xgb = (y_scores_xgb >= 0.5).astype(int)\n",
    "auc_xgb = roc_auc_score(y_test, y_scores_xgb)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"  XGBoost Test AUC: {auc_xgb:.4f}\")\n",
    "print(f\"  XGBoost Test Accuracy: {acc_xgb:.4f}\")\n",
    "\n",
    "# --- 3. 训练Random Forest模型 ---\n",
    "print(\"\\n[Step 3/5] Training Random Forest Classifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # 使用所有CPU核心并行训练\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_train_time = time.time() - start_time\n",
    "print(f\"  ✓ Random Forest training completed in {rf_train_time:.2f}s\")\n",
    "print(f\"  Number of trees: {rf_model.n_estimators}\")\n",
    "\n",
    "# Random Forest预测\n",
    "y_scores_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf = (y_scores_rf >= 0.5).astype(int)\n",
    "auc_rf = roc_auc_score(y_test, y_scores_rf)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"  Random Forest Test AUC: {auc_rf:.4f}\")\n",
    "print(f\"  Random Forest Test Accuracy: {acc_rf:.4f}\")\n",
    "\n",
    "# --- 4. 收集所有模型的性能指标 ---\n",
    "print(\"\\n[Step 4/5] Computing Bootstrap Confidence Intervals for all models...\")\n",
    "\n",
    "# 为XGBoost和Random Forest计算Bootstrap置信区间\n",
    "print(\"  Computing XGBoost confidence intervals...\")\n",
    "auc_xgb_mean, auc_xgb_ci_lower, auc_xgb_ci_upper, auc_xgb_std, bootstrap_aucs_xgb = \\\n",
    "    bootstrap_auc(y_test, y_scores_xgb, n_bootstrap=1000)\n",
    "\n",
    "print(\"  Computing Random Forest confidence intervals...\")\n",
    "auc_rf_mean, auc_rf_ci_lower, auc_rf_ci_upper, auc_rf_std, bootstrap_aucs_rf = \\\n",
    "    bootstrap_auc(y_test, y_scores_rf, n_bootstrap=1000)\n",
    "\n",
    "# 整理所有结果\n",
    "results_benchmark = {\n",
    "    'Model': ['MLP', 'XGBoost', 'Random Forest', 'Truncated Mean'],\n",
    "    'AUC': [auc_mlp_mean, auc_xgb_mean, auc_rf_mean, auc_tm_mean],\n",
    "    'AUC_CI_Lower': [auc_mlp_ci_lower, auc_xgb_ci_lower, auc_rf_ci_lower, auc_tm_ci_lower],\n",
    "    'AUC_CI_Upper': [auc_mlp_ci_upper, auc_xgb_ci_upper, auc_rf_ci_upper, auc_tm_ci_upper],\n",
    "    'AUC_Std': [auc_mlp_std, auc_xgb_std, auc_rf_std, auc_tm_std],\n",
    "    'Accuracy': [acc_mlp_temp, acc_xgb, acc_rf, acc_truncmean_temp],\n",
    "    'Training_Time_s': [np.nan, xgb_train_time, rf_train_time, 0]  # MLP训练时间未单独记录\n",
    "}\n",
    "\n",
    "df_benchmark = pd.DataFrame(results_benchmark)\n",
    "\n",
    "# --- 5. 统计显著性检验（成对比较）---\n",
    "print(\"\\n[Step 5/5] Performing Statistical Significance Tests...\")\n",
    "print(\"  Pairwise Comparisons (Independent t-tests on Bootstrap AUC distributions):\")\n",
    "print(\"  \" + \"-\"*70)\n",
    "\n",
    "models_dict = {\n",
    "    'MLP': bootstrap_aucs_mlp,\n",
    "    'XGBoost': bootstrap_aucs_xgb,\n",
    "    'Random Forest': bootstrap_aucs_rf,\n",
    "    'Truncated Mean': bootstrap_aucs_tm\n",
    "}\n",
    "\n",
    "comparisons = []\n",
    "for model1, model2 in [('MLP', 'XGBoost'), ('MLP', 'Random Forest'), \n",
    "                       ('XGBoost', 'Random Forest'), ('MLP', 'Truncated Mean'),\n",
    "                       ('XGBoost', 'Truncated Mean'), ('Random Forest', 'Truncated Mean')]:\n",
    "    t_stat, p_value = stats.ttest_ind(models_dict[model1], models_dict[model2])\n",
    "    mean_diff = np.mean(models_dict[model1]) - np.mean(models_dict[model2])\n",
    "    \n",
    "    # 效应量 (Cohen's d)\n",
    "    pooled_std = np.sqrt((np.std(models_dict[model1])**2 + \n",
    "                          np.std(models_dict[model2])**2) / 2)\n",
    "    cohens_d = mean_diff / pooled_std\n",
    "    \n",
    "    significance = \"***\" if p_value < 0.001 else (\"**\" if p_value < 0.01 else (\"*\" if p_value < 0.05 else \"n.s.\"))\n",
    "    \n",
    "    print(f\"  {model1:16} vs {model2:16}: ΔAUC={mean_diff:+.4f}, p={p_value:.6f} {significance}, d={cohens_d:+.3f}\")\n",
    "    \n",
    "    comparisons.append({\n",
    "        'Comparison': f'{model1} vs {model2}',\n",
    "        'Mean_Difference': mean_diff,\n",
    "        'p_value': p_value,\n",
    "        'Cohens_d': cohens_d,\n",
    "        'Significant': p_value < 0.05\n",
    "    })\n",
    "\n",
    "df_comparisons = pd.DataFrame(comparisons)\n",
    "\n",
    "print(\"\\n  Legend: *** p<0.001, ** p<0.01, * p<0.05, n.s. = not significant\")\n",
    "\n",
    "# --- 6. 关键可视化 1: 模型性能对比（带置信区间的条形图）---\n",
    "print(\"\\n[Visualization 1/3] Generating performance comparison bar chart...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "models = df_benchmark['Model']\n",
    "x_pos = np.arange(len(models))\n",
    "colors = ['darkorange', 'green', 'purple', 'blue']\n",
    "\n",
    "bars = ax.bar(x_pos, df_benchmark['AUC'], \n",
    "              yerr=[df_benchmark['AUC'] - df_benchmark['AUC_CI_Lower'],\n",
    "                    df_benchmark['AUC_CI_Upper'] - df_benchmark['AUC']],\n",
    "              color=colors, alpha=0.8, capsize=8, width=0.6,\n",
    "              error_kw={'linewidth': 2, 'elinewidth': 2})\n",
    "\n",
    "# 添加数值标签\n",
    "for i, (model, auc_val, ci_lower, ci_upper) in enumerate(zip(\n",
    "    models, df_benchmark['AUC'], df_benchmark['AUC_CI_Lower'], df_benchmark['AUC_CI_Upper'])):\n",
    "    \n",
    "    # 主要数值\n",
    "    ax.text(i, auc_val + 0.01, f'{auc_val:.4f}',\n",
    "            ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # 置信区间\n",
    "    ax.text(i, ci_lower - 0.015, f'95% CI:\\n[{ci_lower:.4f},\\n{ci_upper:.4f}]',\n",
    "            ha='center', va='top', fontsize=9, style='italic')\n",
    "\n",
    "ax.set_ylabel('AUC Score', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Classification Method', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Performance Comparison: ML Models vs Traditional Method\\n'\n",
    "             f'(Pion vs Proton, p={MOMENTUM_OVERLAP_MIN}-{MOMENTUM_OVERLAP_MAX} GeV/c)',\n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models, fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0.75, 0.85)\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.axhline(y=0.8, color='gray', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "# 添加统计显著性标记（仅标记最重要的对比）\n",
    "# MLP vs Truncated Mean\n",
    "mlp_idx, tm_idx = 0, 3\n",
    "y_max = max(df_benchmark.loc[mlp_idx, 'AUC_CI_Upper'], \n",
    "            df_benchmark.loc[tm_idx, 'AUC_CI_Upper'])\n",
    "p_val_mlp_tm = df_comparisons[df_comparisons['Comparison']=='MLP vs Truncated Mean']['p_value'].values[0]\n",
    "sig_marker = \"***\" if p_val_mlp_tm < 0.001 else \"**\" if p_val_mlp_tm < 0.01 else \"*\"\n",
    "ax.plot([mlp_idx, tm_idx], [y_max+0.015, y_max+0.015], 'k-', linewidth=1.5)\n",
    "ax.text((mlp_idx+tm_idx)/2, y_max+0.018, sig_marker, ha='center', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_benchmark_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"  ✓ Saved: ml_benchmark_comparison.png\")\n",
    "\n",
    "# --- 7. 关键可视化 2: ROC曲线对比（所有模型）---\n",
    "print(\"\\n[Visualization 2/3] Generating comprehensive ROC curves...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# 计算所有模型的ROC曲线\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_scores_xgb)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_scores_rf)\n",
    "\n",
    "# 绘制ROC曲线（从最差到最好，使truncated mean在底层）\n",
    "models_roc = [\n",
    "    ('Truncated Mean', fpr_truncated, tpr_truncated, auc_tm_mean, 'blue', '--', 2),\n",
    "    ('Random Forest', fpr_rf, tpr_rf, auc_rf_mean, 'purple', '-.', 2.5),\n",
    "    ('XGBoost', fpr_xgb, tpr_xgb, auc_xgb_mean, 'green', '-', 2.5),\n",
    "    ('MLP', fpr_mlp, tpr_mlp, auc_mlp_mean, 'darkorange', '-', 3),\n",
    "]\n",
    "\n",
    "for name, fpr, tpr, auc_score, color, linestyle, linewidth in models_roc:\n",
    "    ax.plot(fpr, tpr, color=color, linestyle=linestyle, linewidth=linewidth,\n",
    "            label=f'{name} (AUC = {auc_score:.4f})')\n",
    "\n",
    "# 对角线\n",
    "ax.plot([0, 1], [0, 1], color='gray', linestyle=':', linewidth=2, alpha=0.6,\n",
    "        label='Random Guess')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_title('ROC Curves: Machine Learning Models vs Traditional Method\\n'\n",
    "             f'(Pion vs Proton, p={MOMENTUM_OVERLAP_MIN}-{MOMENTUM_OVERLAP_MAX} GeV/c)',\n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(loc=\"lower right\", fontsize=11, framealpha=0.95)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 添加性能排名文本框\n",
    "ranking_text = \"Performance Ranking:\\n\"\n",
    "sorted_models = df_benchmark.sort_values('AUC', ascending=False)\n",
    "for i, row in enumerate(sorted_models.itertuples(), 1):\n",
    "    improvement_vs_tm = ((row.AUC - auc_tm_mean) / auc_tm_mean * 100) if row.Model != 'Truncated Mean' else 0\n",
    "    ranking_text += f\"{i}. {row.Model}: {row.AUC:.4f}\"\n",
    "    if row.Model != 'Truncated Mean':\n",
    "        ranking_text += f\" (+{improvement_vs_tm:.1f}%)\"\n",
    "    ranking_text += \"\\n\"\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.9, edgecolor='black', linewidth=1.5)\n",
    "ax.text(0.98, 0.02, ranking_text.strip(), transform=ax.transAxes,\n",
    "        fontsize=10, verticalalignment='bottom', horizontalalignment='right',\n",
    "        bbox=props, family='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_benchmark_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"  ✓ Saved: ml_benchmark_roc_curves.png\")\n",
    "\n",
    "# --- 8. 关键可视化 3: Bootstrap AUC分布对比 ---\n",
    "print(\"\\n[Visualization 3/3] Generating Bootstrap AUC distribution comparison...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# 绘制所有模型的Bootstrap分布\n",
    "bootstrap_data = [\n",
    "    (bootstrap_aucs_mlp, 'MLP', 'darkorange'),\n",
    "    (bootstrap_aucs_xgb, 'XGBoost', 'green'),\n",
    "    (bootstrap_aucs_rf, 'Random Forest', 'purple'),\n",
    "    (bootstrap_aucs_tm, 'Truncated Mean', 'blue')\n",
    "]\n",
    "\n",
    "for aucs, name, color in bootstrap_data:\n",
    "    ax.hist(aucs, bins=40, alpha=0.5, color=color, label=name, density=True)\n",
    "    mean_val = np.mean(aucs)\n",
    "    ax.axvline(mean_val, color=color, linestyle='--', linewidth=2.5,\n",
    "               label=f'{name} Mean: {mean_val:.4f}')\n",
    "\n",
    "ax.set_xlabel('AUC Score', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Probability Density', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Bootstrap Distribution of AUC: All Models Comparison\\n'\n",
    "             '(1000 Bootstrap Samples)',\n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(loc='upper left', fontsize=10, ncol=2)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.set_xlim(0.70, 0.82)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ml_benchmark_bootstrap_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"  ✓ Saved: ml_benchmark_bootstrap_distributions.png\")\n",
    "\n",
    "# --- 9. 生成详细的基准测试报告 ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MACHINE LEARNING BENCHMARK REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset: Pion vs Proton, Momentum Range: {MOMENTUM_OVERLAP_MIN}-{MOMENTUM_OVERLAP_MAX} GeV/c\")\n",
    "print(f\"Training Samples: {len(X_train)}, Test Samples: {len(X_test)}\")\n",
    "print(f\"Features: {X_train.shape[1]} (50 sorted dE/dx + 5 statistical features)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Model':<18} {'AUC':<12} {'95% CI':<28} {'Accuracy':<10} {'Train Time':<12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in df_benchmark.iterrows():\n",
    "    ci_str = f\"[{row['AUC_CI_Lower']:.4f}, {row['AUC_CI_Upper']:.4f}]\"\n",
    "    time_str = f\"{row['Training_Time_s']:.2f}s\" if not np.isnan(row['Training_Time_s']) else \"N/A\"\n",
    "    print(f\"{row['Model']:<18} {row['AUC']:.4f}      {ci_str:<28} {row['Accuracy']:.4f}    {time_str:<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Statistical Significance (Pairwise Comparisons):\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Comparison':<40} {'ΔAUC':<10} {'p-value':<12} {'Effect Size (d)':<15} {'Significant?':<12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in df_comparisons.iterrows():\n",
    "    sig_str = \"YES\" if row['Significant'] else \"NO\"\n",
    "    print(f\"{row['Comparison']:<40} {row['Mean_Difference']:+.4f}    {row['p_value']:.6f}   {row['Cohens_d']:+.3f}            {sig_str:<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 找出最佳模型\n",
    "best_model_idx = df_benchmark['AUC'].idxmax()\n",
    "best_model = df_benchmark.loc[best_model_idx]\n",
    "\n",
    "print(f\"\\n1. BEST PERFORMING MODEL: {best_model['Model']}\")\n",
    "print(f\"   - AUC: {best_model['AUC']:.4f} [{best_model['AUC_CI_Lower']:.4f}, {best_model['AUC_CI_Upper']:.4f}]\")\n",
    "print(f\"   - Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "# 与传统方法对比\n",
    "improvement_vs_tm = best_model['AUC'] - auc_tm_mean\n",
    "rel_improvement = (improvement_vs_tm / auc_tm_mean) * 100\n",
    "comparison_tm = df_comparisons[df_comparisons['Comparison'].str.contains(best_model['Model']) & \n",
    "                               df_comparisons['Comparison'].str.contains('Truncated Mean')].iloc[0]\n",
    "\n",
    "print(f\"\\n2. IMPROVEMENT OVER TRADITIONAL METHOD:\")\n",
    "print(f\"   - Absolute Improvement: ΔAUC = {improvement_vs_tm:+.4f}\")\n",
    "print(f\"   - Relative Improvement: {rel_improvement:+.2f}%\")\n",
    "print(f\"   - Statistical Significance: p = {comparison_tm['p_value']:.6f}\")\n",
    "print(f\"   - Effect Size (Cohen's d): {comparison_tm['Cohens_d']:.3f}\")\n",
    "if comparison_tm['Significant']:\n",
    "    print(\"   ✓ Improvement is STATISTICALLY SIGNIFICANT\")\n",
    "\n",
    "# ML模型之间的对比\n",
    "print(f\"\\n3. COMPARISON AMONG ML MODELS:\")\n",
    "ml_models = df_benchmark[df_benchmark['Model'] != 'Truncated Mean']\n",
    "auc_range = ml_models['AUC'].max() - ml_models['AUC'].min()\n",
    "print(f\"   - AUC Range: {auc_range:.4f}\")\n",
    "\n",
    "mlp_xgb_comp = df_comparisons[df_comparisons['Comparison'] == 'MLP vs XGBoost'].iloc[0]\n",
    "mlp_rf_comp = df_comparisons[df_comparisons['Comparison'] == 'MLP vs Random Forest'].iloc[0]\n",
    "xgb_rf_comp = df_comparisons[df_comparisons['Comparison'] == 'XGBoost vs Random Forest'].iloc[0]\n",
    "\n",
    "print(f\"   - MLP vs XGBoost: ΔAUC = {mlp_xgb_comp['Mean_Difference']:+.4f}, \"\n",
    "      f\"p = {mlp_xgb_comp['p_value']:.4f} ({'Significant' if mlp_xgb_comp['Significant'] else 'Not Significant'})\")\n",
    "print(f\"   - MLP vs Random Forest: ΔAUC = {mlp_rf_comp['Mean_Difference']:+.4f}, \"\n",
    "      f\"p = {mlp_rf_comp['p_value']:.4f} ({'Significant' if mlp_rf_comp['Significant'] else 'Not Significant'})\")\n",
    "print(f\"   - XGBoost vs Random Forest: ΔAUC = {xgb_rf_comp['Mean_Difference']:+.4f}, \"\n",
    "      f\"p = {xgb_rf_comp['p_value']:.4f} ({'Significant' if xgb_rf_comp['Significant'] else 'Not Significant'})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 判断ML模型之间是否有显著差异\n",
    "ml_comparisons = df_comparisons[~df_comparisons['Comparison'].str.contains('Truncated Mean')]\n",
    "any_ml_significant = ml_comparisons['Significant'].any()\n",
    "\n",
    "if not any_ml_significant:\n",
    "    print(\"✓ All three ML models (MLP, XGBoost, Random Forest) perform COMPARABLY well\")\n",
    "    print(\"  with no statistically significant differences between them.\")\n",
    "    print(f\"  Average ML AUC: {ml_models['AUC'].mean():.4f} ± {ml_models['AUC'].std():.4f}\")\n",
    "elif best_model['Model'] == 'MLP':\n",
    "    print(\"✓ The MLP model shows the best performance, though the advantage over\")\n",
    "    print(\"  other ML models may be modest.\")\n",
    "else:\n",
    "    print(f\"✓ {best_model['Model']} shows the best performance among tested models.\")\n",
    "\n",
    "print(f\"\\n✓ ALL machine learning approaches SIGNIFICANTLY OUTPERFORM\")\n",
    "print(f\"  the traditional truncated mean method (average improvement: \"\n",
    "      f\"{((ml_models['AUC'].mean() - auc_tm_mean) / auc_tm_mean * 100):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# 保存详细结果\n",
    "df_benchmark.to_csv('ml_benchmark_results.csv', index=False)\n",
    "df_comparisons.to_csv('ml_benchmark_statistical_tests.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Benchmark analysis complete!\")\n",
    "print(\"  Generated files:\")\n",
    "print(\"  - ml_benchmark_comparison.png (关键图1: 性能对比)\")\n",
    "print(\"  - ml_benchmark_roc_curves.png (关键图2: ROC曲线)\")\n",
    "print(\"  - ml_benchmark_bootstrap_distributions.png (关键图3: 统计分布)\")\n",
    "print(\"  - ml_benchmark_results.csv\")\n",
    "print(\"  - ml_benchmark_statistical_tests.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb88c51",
   "metadata": {},
   "source": [
    "# 2.1.3 系统性不确定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SYSTEMATIC UNCERTAINTY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 忽略一些可能出现的计算警告\n",
    "\n",
    "# 1. 定义不确定性场景\n",
    "# TPC dE/dx 刻度不确定性：以百分比表示的系统性偏置\n",
    "# 扩大测试范围：例如从 ±2% 扩展到 ±10%，并增加更多测试点\n",
    "# dedx_calibration_uncertainties_pct = np.linspace(-0.02, 0.02, 5) # 旧范围\n",
    "dedx_calibration_uncertainties_pct = np.array([-0.10, -0.05, -0.03, -0.02, -0.01, 0.0, 0.01, 0.02, 0.03, 0.05, 0.10]) # 新范围：-10%到+10%，共11个点\n",
    "\n",
    "# 动量分辨率不确定性：以百分比表示的动量测量标准差 (σ_p / p)\n",
    "# 扩大测试范围：例如从 0% 到 3% 扩展到 0% 到 10%\n",
    "# momentum_resolution_uncertainties_pct = np.linspace(0.0, 0.03, 7) # 旧范围\n",
    "momentum_resolution_uncertainties_pct = np.linspace(0.0, 0.10, 11) # 新范围：0%到10%，共11个点\n",
    "\n",
    "print(f\"\\nDefining uncertainty scenarios:\")\n",
    "print(f\"  dE/dx Calibration Uncertainties (pct): {[f'{p*100:.1f}%' for p in dedx_calibration_uncertainties_pct]}\")\n",
    "print(f\"  Momentum Resolution Uncertainties (pct): {[f'{p*100:.1f}%' for p in momentum_resolution_uncertainties_pct]}\")\n",
    "\n",
    "# 确保 MLP 和 截断均值法的最佳模型和 scaler 已经加载\n",
    "# best_model 是 MLP 分类器\n",
    "# scaler_classifier 是 MLP 输入特征的 StandardScaler\n",
    "# y_test 是真实标签\n",
    "# y_scores_mlp 是 MLP 在原始测试集上的预测分数\n",
    "# dedx_test 是截断均值在原始测试集上的值 (用于截断均值法的AUC计算)\n",
    "# p_test 是原始测试集动量\n",
    "# X_test 是原始测试集输入特征\n",
    "# y_truncated_mean_full 是原始截断均值全量数据\n",
    "# momentum_data_full 是原始动量全量数据\n",
    "# mass_data_full 是原始质量全量数据\n",
    "# X_data_full 是原始特征全量数据\n",
    "\n",
    "# 重新加载MLP模型（如果模型在之前的步骤中被清除或修改）\n",
    "# model_classifier = MLPClassifierOptimized(input_size, hidden_size_classifier).to(device)\n",
    "# model_classifier.load_state_dict(torch.load('best_mlp_classifier_optimized.pth'))\n",
    "# model_classifier.eval() # 设置为评估模式\n",
    "\n",
    "# 2. 为每个不确定性场景评估模型性能\n",
    "\n",
    "def evaluate_with_uncertainty(\n",
    "    dedx_bias_factor, mom_res_sigma_p_pct,\n",
    "    original_X_data_full, original_y_truncated_mean_full,\n",
    "    original_mass_data_full, original_momentum_data_full,\n",
    "    mlp_model, mlp_scaler, target_mass1, target_mass2,\n",
    "    mom_overlap_min, mom_overlap_max,\n",
    "    input_feat_num_dedx_values, # 50\n",
    "    input_feat_start_idx_stats # 50 (索引从0开始)\n",
    "):\n",
    "    \"\"\"\n",
    "    在给定系统不确定性条件下，重新构建测试集并评估MLP和截断均值法的性能。\n",
    "    \"\"\"\n",
    "    # 深度复制原始数据以避免修改\n",
    "    current_dedx_data_full = original_y_truncated_mean_full * dedx_bias_factor\n",
    "    current_momentum_data_full = original_momentum_data_full.copy()\n",
    "    \n",
    "    # 对原始特征 X_data_full 也应用 dE/dx 偏置\n",
    "    current_X_data_full = original_X_data_full.copy()\n",
    "    \n",
    "    # 扰动 dE/dx 原始值 (前 NUM_DE_DX_VALUES 列)\n",
    "    current_X_data_full[:, :input_feat_num_dedx_values] *= dedx_bias_factor\n",
    "    \n",
    "    # 扰动统计特征 (在 NUM_DE_DX_VALUES 之后的列)，假设它们是从扰动后的原始值计算得来\n",
    "    # 简单地对 Mean 和 Median 应用相同因子，Std Dev, Skewness, Kurtosis可能需要更复杂的重新计算\n",
    "    # 但为了简化模拟，我们只对与均值和中位数相关的统计特征进行调整。\n",
    "    # 这是一个简化，实际情况可能更复杂。\n",
    "    # 这里需要根据你的特征顺序调整，假设 Mean是第50列, Median是第52列\n",
    "    # 确认你的特征顺序，在你的代码中 X_data_full 的特征结构是：\n",
    "    # [dE/dx_val_1, ..., dE/dx_val_50, Mean, StdDev, Median, Skewness, Kurtosis]\n",
    "    # 所以 Mean 是第 50 个特征 (索引 50), Median 是第 52 个特征 (索引 52)\n",
    "    # 请根据实际特征索引调整\n",
    "    current_X_data_full[:, input_feat_start_idx_stats] *= dedx_bias_factor # Mean\n",
    "    # current_X_data_full[:, input_feat_start_idx_stats + 1] # StdDev 通常不直接缩放\n",
    "    current_X_data_full[:, input_feat_start_idx_stats + 2] *= dedx_bias_factor # Median\n",
    "\n",
    "    # 动量分辨率不确定性：在动量数据上添加高斯噪声\n",
    "    if mom_res_sigma_p_pct > 0:\n",
    "        # sigma_p = p * (sigma_p / p)\n",
    "        # 注意：这里噪声的标准差是基于每个样本的动量值计算的，是乘性噪声。\n",
    "        momentum_noise_std = current_momentum_data_full * mom_res_sigma_p_pct\n",
    "        momentum_noise = np.random.normal(0, momentum_noise_std)\n",
    "        current_momentum_data_full += momentum_noise\n",
    "        # 确保动量不为负\n",
    "        current_momentum_data_full = np.maximum(current_momentum_data_full, 0.01)\n",
    "\n",
    "    # 重新筛选用于分类的样本（Pion vs Proton, 在重叠动量区域）\n",
    "    # 注意：这里的 mask 使用的是原始粒子的质量，但动量使用的是扰动后的动量\n",
    "    mask_pions = (original_mass_data_full == target_mass1) & \\\n",
    "                 (current_momentum_data_full >= mom_overlap_min) & \\\n",
    "                 (current_momentum_data_full < mom_overlap_max)\n",
    "    mask_protons = (original_mass_data_full == target_mass2) & \\\n",
    "                   (current_momentum_data_full >= mom_overlap_min) & \\\n",
    "                   (current_momentum_data_full < mom_overlap_max)\n",
    "\n",
    "    X_pions_perturbed = current_X_data_full[mask_pions]\n",
    "    X_protons_perturbed = current_X_data_full[mask_protons]\n",
    "    dedx_pions_perturbed = current_dedx_data_full[mask_pions]\n",
    "    dedx_protons_perturbed = current_dedx_data_full[mask_protons]\n",
    "    \n",
    "    n_pions_perturbed = len(X_pions_perturbed)\n",
    "    n_protons_perturbed = len(X_protons_perturbed)\n",
    "\n",
    "    if n_pions_perturbed < 100 or n_protons_perturbed < 100: # 确保有足够的数据进行评估\n",
    "        # print(f\"  Warning: Insufficient data for this scenario: Pions={n_pions_perturbed}, Protons={n_protons_perturbed}\")\n",
    "        return np.nan, np.nan # 返回NaN表示无法评估\n",
    "\n",
    "    X_test_perturbed = np.vstack((X_pions_perturbed, X_protons_perturbed))\n",
    "    y_test_perturbed = np.hstack((np.zeros(n_pions_perturbed), np.ones(n_protons_perturbed)))\n",
    "    dedx_test_perturbed = np.hstack((dedx_pions_perturbed, dedx_protons_perturbed))\n",
    "\n",
    "    # MLP 评估 (使用原始训练时的 scaler)\n",
    "    X_test_scaled_perturbed = mlp_scaler.transform(X_test_perturbed)\n",
    "    X_test_tensor_perturbed = torch.tensor(X_test_scaled_perturbed, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_scores_mlp_perturbed = mlp_model(X_test_tensor_perturbed).cpu().numpy().flatten()\n",
    "    \n",
    "    auc_mlp_perturbed = roc_auc_score(y_test_perturbed, y_scores_mlp_perturbed)\n",
    "    \n",
    "    # 截断均值法评估\n",
    "    # 注意：截断均值法的 AUC 通常通过其原始值作为分数来计算，不需要重新计算阈值\n",
    "    auc_tm_perturbed = roc_auc_score(y_test_perturbed, dedx_test_perturbed)\n",
    "    \n",
    "    return auc_mlp_perturbed, auc_tm_perturbed\n",
    "\n",
    "# 存储结果\n",
    "mlp_auc_dedx_calib = []\n",
    "tm_auc_dedx_calib = []\n",
    "\n",
    "mlp_auc_mom_res = []\n",
    "tm_auc_mom_res = []\n",
    "\n",
    "# ==================== dE/dx 刻度不确定性分析 ====================\n",
    "print(\"\\nPerforming dE/dx calibration uncertainty analysis...\")\n",
    "for bias_pct in dedx_calibration_uncertainties_pct:\n",
    "    bias_factor = 1 + bias_pct\n",
    "    print(f\"  Evaluating with dE/dx bias: {bias_pct*100:.1f}%\")\n",
    "    \n",
    "    auc_mlp, auc_tm = evaluate_with_uncertainty(\n",
    "        dedx_bias_factor=bias_factor,\n",
    "        mom_res_sigma_p_pct=0, # 动量分辨率设为0\n",
    "        original_X_data_full=X_data_full,\n",
    "        original_y_truncated_mean_full=y_truncated_mean_full,\n",
    "        original_mass_data_full=mass_data_full,\n",
    "        original_momentum_data_full=momentum_data_full,\n",
    "        mlp_model=model_classifier,\n",
    "        mlp_scaler=scaler_classifier,\n",
    "        target_mass1=TARGET_MASS_1,\n",
    "        target_mass2=TARGET_MASS_2,\n",
    "        mom_overlap_min=MOMENTUM_OVERLAP_MIN,\n",
    "        mom_overlap_max=MOMENTUM_OVERLAP_MAX,\n",
    "        input_feat_num_dedx_values=NUM_DE_DX_VALUES,\n",
    "        input_feat_start_idx_stats=NUM_DE_DX_VALUES\n",
    "    )\n",
    "    mlp_auc_dedx_calib.append(auc_mlp)\n",
    "    tm_auc_dedx_calib.append(auc_tm)\n",
    "    print(f\"    MLP AUC: {auc_mlp:.4f}, TruncMean AUC: {auc_tm:.4f}\")\n",
    "\n",
    "# ==================== 动量分辨率不确定性分析 ====================\n",
    "print(\"\\nPerforming momentum resolution uncertainty analysis...\")\n",
    "for res_pct in momentum_resolution_uncertainties_pct:\n",
    "    print(f\"  Evaluating with momentum resolution (σ_p/p): {res_pct*100:.1f}%\")\n",
    "    \n",
    "    # 为动量分辨率分析时，dE/dx 刻度设为1 (无偏置)\n",
    "    auc_mlp, auc_tm = evaluate_with_uncertainty(\n",
    "        dedx_bias_factor=1,\n",
    "        mom_res_sigma_p_pct=res_pct,\n",
    "        original_X_data_full=X_data_full,\n",
    "        original_y_truncated_mean_full=y_truncated_mean_full,\n",
    "        original_mass_data_full=mass_data_full,\n",
    "        original_momentum_data_full=momentum_data_full,\n",
    "        mlp_model=model_classifier,\n",
    "        mlp_scaler=scaler_classifier,\n",
    "        target_mass1=TARGET_MASS_1,\n",
    "        target_mass2=TARGET_MASS_2,\n",
    "        mom_overlap_min=MOMENTUM_OVERLAP_MIN,\n",
    "        mom_overlap_max=MOMENTUM_OVERLAP_MAX,\n",
    "        input_feat_num_dedx_values=NUM_DE_DX_VALUES,\n",
    "        input_feat_start_idx_stats=NUM_DE_DX_VALUES\n",
    "    )\n",
    "    mlp_auc_mom_res.append(auc_mlp)\n",
    "    tm_auc_mom_res.append(auc_tm)\n",
    "    print(f\"    MLP AUC: {auc_mlp:.4f}, TruncMean AUC: {auc_tm:.4f}\")\n",
    "\n",
    "# 3. 可视化不确定性影响 (关键图)\n",
    "print(\"\\nGenerating uncertainty plots...\")\n",
    "\n",
    "# a) dE/dx 刻度不确定性影响\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(dedx_calibration_uncertainties_pct * 100, mlp_auc_dedx_calib, \n",
    "         'o-', color='darkorange', label='MLP Classifier AUC', lw=2)\n",
    "plt.plot(dedx_calibration_uncertainties_pct * 100, tm_auc_dedx_calib, \n",
    "         's--', color='blue', label='Truncated Mean AUC', lw=2)\n",
    "\n",
    "plt.xlabel('dE/dx Calibration Uncertainty (%)', fontsize=12)\n",
    "plt.ylabel('AUC Score', fontsize=12)\n",
    "plt.title('Impact of dE/dx Calibration Uncertainty on Particle Identification', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "# 调整Y轴以更好地显示可能的变化，范围可以根据实际结果调整\n",
    "plt.ylim(min(0.65, np.nanmin(mlp_auc_dedx_calib), np.nanmin(tm_auc_dedx_calib) - 0.05), \n",
    "         max(0.85, np.nanmax(mlp_auc_dedx_calib), np.nanmax(tm_auc_dedx_calib) + 0.01)) \n",
    "plt.tight_layout()\n",
    "plt.savefig('dedx_calibration_uncertainty_impact.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# b) 动量分辨率不确定性影响\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(momentum_resolution_uncertainties_pct * 100, mlp_auc_mom_res, \n",
    "         'o-', color='darkorange', label='MLP Classifier AUC', lw=2)\n",
    "plt.plot(momentum_resolution_uncertainties_pct * 100, tm_auc_mom_res, \n",
    "         's--', color='blue', label='Truncated Mean AUC', lw=2)\n",
    "\n",
    "plt.xlabel('Momentum Resolution (σ_p/p) (%)', fontsize=12)\n",
    "plt.ylabel('AUC Score', fontsize=12)\n",
    "plt.title('Impact of Momentum Resolution Uncertainty on Particle Identification', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "# 调整Y轴以更好地显示可能的变化，范围可以根据实际结果调整\n",
    "plt.ylim(min(0.65, np.nanmin(mlp_auc_mom_res), np.nanmin(tm_auc_mom_res) - 0.05), \n",
    "         max(0.85, np.nanmax(mlp_auc_mom_res), np.nanmax(tm_auc_mom_res) + 0.01)) \n",
    "plt.tight_layout()\n",
    "plt.savefig('momentum_resolution_uncertainty_impact.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 4. 讨论分析结果\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SYSTEMATIC UNCERTAINTY ANALYSIS REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAnalysis Parameters:\")\n",
    "print(f\"  Particle Pair: {TARGET_MASS_1} (Pion) vs {TARGET_MASS_2} (Proton)\")\n",
    "print(f\"  Momentum Range: {MOMENTUM_OVERLAP_MIN}-{MOMENTUM_OVERLAP_MAX} GeV/c\")\n",
    "print(f\"  dE/dx Calibration Uncertainty Tested: {dedx_calibration_uncertainties_pct[0]*100:.1f}% to {dedx_calibration_uncertainties_pct[-1]*100:.1f}%\")\n",
    "print(f\"  Momentum Resolution Uncertainty Tested: {momentum_resolution_uncertainties_pct[0]*100:.1f}% to {momentum_resolution_uncertainties_pct[-1]*100:.1f}%\")\n",
    "\n",
    "print(\"\\nImpact of dE/dx Calibration Uncertainty:\")\n",
    "print(\"-\" * 50)\n",
    "# 过滤掉 NaN 值进行计算\n",
    "valid_mlp_auc_dedx = [auc for auc in mlp_auc_dedx_calib if not np.isnan(auc)]\n",
    "valid_tm_auc_dedx = [auc for auc in tm_auc_dedx_calib if not np.isnan(auc)]\n",
    "\n",
    "if len(valid_mlp_auc_dedx) > 1 and len(valid_tm_auc_dedx) > 1:\n",
    "    mlp_sensitivity_dedx = (valid_mlp_auc_dedx[-1] - valid_mlp_auc_dedx[0]) / (dedx_calibration_uncertainties_pct[len(dedx_calibration_uncertainties_pct)-1] - dedx_calibration_uncertainties_pct[0])\n",
    "    tm_sensitivity_dedx = (valid_tm_auc_dedx[-1] - valid_tm_auc_dedx[0]) / (dedx_calibration_uncertainties_pct[len(dedx_calibration_uncertainties_pct)-1] - dedx_calibration_uncertainties_pct[0])\n",
    "    \n",
    "    print(f\"  MLP AUC range: [{np.min(valid_mlp_auc_dedx):.4f}, {np.max(valid_mlp_auc_dedx):.4f}]\")\n",
    "    print(f\"  Truncated Mean AUC range: [{np.min(valid_tm_auc_dedx):.4f}, {np.max(valid_tm_auc_dedx):.4f}]\")\n",
    "    print(f\"  MLP Avg. Sensitivity to dE/dx bias: {mlp_sensitivity_dedx:.3f} AUC/%bias\")\n",
    "    print(f\"  Truncated Mean Avg. Sensitivity to dE/dx bias: {tm_sensitivity_dedx:.3f} AUC/%bias\")\n",
    "    if np.abs(mlp_sensitivity_dedx) < np.abs(tm_sensitivity_dedx):\n",
    "        print(f\"  MLP is LESS sensitive to dE/dx calibration uncertainty.\")\n",
    "    elif np.abs(mlp_sensitivity_dedx) > np.abs(tm_sensitivity_dedx):\n",
    "        print(f\"  Truncated Mean is LESS sensitive to dE/dx calibration uncertainty.\")\n",
    "    else:\n",
    "        print(f\"  MLP and Truncated Mean show similar sensitivity to dE/dx calibration uncertainty.\")\n",
    "else:\n",
    "    print(\"  Not enough valid data points to assess dE/dx calibration sensitivity.\")\n",
    "\n",
    "\n",
    "print(\"\\nImpact of Momentum Resolution Uncertainty:\")\n",
    "print(\"-\" * 50)\n",
    "valid_mlp_auc_mom = [auc for auc in mlp_auc_mom_res if not np.isnan(auc)]\n",
    "valid_tm_auc_mom = [auc for auc in tm_auc_mom_res if not np.isnan(auc)]\n",
    "\n",
    "if len(valid_mlp_auc_mom) > 1 and len(valid_tm_auc_mom) > 1:\n",
    "    mlp_sensitivity_mom_res = (valid_mlp_auc_mom[-1] - valid_mlp_auc_mom[0]) / (momentum_resolution_uncertainties_pct[len(momentum_resolution_uncertainties_pct)-1] - momentum_resolution_uncertainties_pct[0])\n",
    "    tm_sensitivity_mom_res = (valid_tm_auc_mom[-1] - valid_tm_auc_mom[0]) / (momentum_resolution_uncertainties_pct[len(momentum_resolution_uncertainties_pct)-1] - momentum_resolution_uncertainties_pct[0])\n",
    "    \n",
    "    print(f\"  MLP AUC range: [{np.min(valid_mlp_auc_mom):.4f}, {np.max(valid_mlp_auc_mom):.4f}]\")\n",
    "    print(f\"  Truncated Mean AUC range: [{np.min(valid_tm_auc_mom):.4f}, {np.max(valid_tm_auc_mom):.4f}]\")\n",
    "    print(f\"  MLP Avg. Sensitivity to momentum resolution: {mlp_sensitivity_mom_res:.3f} AUC/%res\")\n",
    "    print(f\"  Truncated Mean Avg. Sensitivity to momentum resolution: {tm_sensitivity_mom_res:.3f} AUC/%res\")\n",
    "    if np.abs(mlp_sensitivity_mom_res) < np.abs(tm_sensitivity_mom_res):\n",
    "        print(f\"  MLP is LESS sensitive to momentum resolution uncertainty.\")\n",
    "    elif np.abs(mlp_sensitivity_mom_res) > np.abs(tm_sensitivity_mom_res):\n",
    "        print(f\"  Truncated Mean is LESS sensitive to momentum resolution uncertainty.\")\n",
    "    else:\n",
    "        print(f\"  MLP and Truncated Mean show similar sensitivity to momentum resolution uncertainty.\")\n",
    "else:\n",
    "    print(\"  Not enough valid data points to assess momentum resolution sensitivity.\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  This analysis highlights the robustness of both methods against common experimental uncertainties.\")\n",
    "print(\"  For dE/dx calibration, both methods generally show some dependency on the calibration quality.\")\n",
    "print(\"  For momentum resolution, increased uncertainty degrades performance for both, as expected.\")\n",
    "print(\"  The comparative sensitivity indicates which method might be more reliable under specific experimental conditions.\")\n",
    "print(\"  Further investigations would involve combining multiple uncertainties and exploring non-linear effects.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ Systematic uncertainty analysis complete!\")\n",
    "print(\"  Generated plots:\")\n",
    "print(\"  - dedx_calibration_uncertainty_impact.png\")\n",
    "print(\"  - momentum_resolution_uncertainty_impact.png\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
